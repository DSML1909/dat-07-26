{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab -- Data Prep & Gradient Boosting\n",
    "\n",
    "Welcome to today's lab!  Today we're going to shift our attention to a more demanding dataset -- the restaurants data.  A quarter million rows, dates, and categorical data make this a more interesting, realistic use case of boosting.  \n",
    "\n",
    "The point of today's lab will be to experiment with different encoding methods and model parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1:**  Load in your dataset, and declare `X` and `y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import category_encoders as ce\n",
    "\n",
    "#df = pd.read_csv('../data/restaurant_data/master.csv', parse_dates=['visit_date'])\n",
    "df = pd.read_csv(r'C:\\Users\\samina\\Desktop\\GA_DATA_SCIENCE\\Data_Sci\\Homework\\Unit3\\data\\insurance_premiums.csv');\n",
    "X = df.drop(['charges'], axis=1)\n",
    "y = df['charges']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2:** Experiment with different encoding methods\n",
    "\n",
    "Let's do a quick check to see how different encoding methods work out of the box on our dataset.\n",
    "\n",
    "You're going to repeat the same process for each of `OrdinalEncoder`, `TargetEncoder`, and `OneHotEncoder` and see which one gives you the best results on our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2a:** Use an `OrdinalEncoder` to transform your training set with the `fit_transform` method.  \n",
    "\n",
    "If you are confused about how the transformation is happening, try using the `mapping` attribute on your category encoder to get a hang of what's going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "ore     = ce.OrdinalEncoder()\n",
    "X1       = ore.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3b:** Initialize a `GradientBoostingRegressor` with the default parameters, fit it on your training set, and score it on your test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8999610643494181"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "gbm = GradientBoostingRegressor()\n",
    "gbm.fit(X1, y)\n",
    "gbm.score(X1, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3c:** Repeat these same steps for the `TargetEncoder` and the `OneHotEncoder`\n",
    "\n",
    "**Important:** The `OneHotEncoder` can take awhile to fit.  If nothing happens in around 4 minutes, just cancel the process and try it again later on when you have more time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\ANACONDA\\lib\\site-packages\\category_encoders\\utils.py:21: FutureWarning: is_categorical is deprecated and will be removed in a future version.  Use is_categorical_dtype instead\n",
      "  elif pd.api.types.is_categorical(cols):\n"
     ]
    }
   ],
   "source": [
    "# for the target encoder\n",
    "te = ce.TargetEncoder()\n",
    "# do your transformations\n",
    "X2 = te.fit_transform(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8976952687334837"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and model fitting\n",
    "gbm = GradientBoostingRegressor()\n",
    "gbm.fit(X2, y)\n",
    "gbm.score(X2, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\ANACONDA\\lib\\site-packages\\category_encoders\\utils.py:21: FutureWarning: is_categorical is deprecated and will be removed in a future version.  Use is_categorical_dtype instead\n",
      "  elif pd.api.types.is_categorical(cols):\n"
     ]
    }
   ],
   "source": [
    "# and for onehot encoding\n",
    "ohe      = ce.OneHotEncoder()\n",
    "X3 = ohe.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8987356739004363"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and look at the model score\n",
    "gbm = GradientBoostingRegressor()\n",
    "\n",
    "gbm.fit(X3, y)\n",
    "gbm.score(X3, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4:** Look at your most important features\n",
    "\n",
    "Similar to the previous lab, take your model's most important features and load them into a dataframe to see what's driving your results.  Use the version of your model that gave you the best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\ANACONDA\\lib\\site-packages\\category_encoders\\utils.py:21: FutureWarning: is_categorical is deprecated and will be removed in a future version.  Use is_categorical_dtype instead\n",
      "  elif pd.api.types.is_categorical(cols):\n"
     ]
    }
   ],
   "source": [
    "# for the target encoder\n",
    "te = ce.TargetEncoder()\n",
    "# do your transformations\n",
    "X  = te.fit_transform(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor()"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and model fitting\n",
    "gbm = GradientBoostingRegressor()\n",
    "gbm.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's look at feature scores\n",
    "feats = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': gbm.feature_importances_\n",
    "}).sort_values(by='Importance', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id</td>\n",
       "      <td>0.872465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>day_of_week</td>\n",
       "      <td>0.104502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>longitude</td>\n",
       "      <td>0.008760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>holiday</td>\n",
       "      <td>0.007499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>latitude</td>\n",
       "      <td>0.004987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>area</td>\n",
       "      <td>0.001253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>genre</td>\n",
       "      <td>0.000535</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Feature  Importance\n",
       "0           id    0.872465\n",
       "1  day_of_week    0.104502\n",
       "6    longitude    0.008760\n",
       "2      holiday    0.007499\n",
       "5     latitude    0.004987\n",
       "4         area    0.001253\n",
       "3        genre    0.000535"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and let's take a look -- two features dominate\n",
    "feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 5:** Can model parameters improve your score?  \n",
    "\n",
    "Take the **best** version of your encoding method and try changing some parameters with your model to see if it improves your score.  \n",
    "\n",
    "You won't have a ton of time to do this, but try some of the following:\n",
    "\n",
    " - Try increasing the number of trees your model uses -- 250, 500, or perhaps more trees if time permits\n",
    " - Try experimenting with differing values for tree depth -- the default is 3, but perhaps 4, 5 or 6 works better\n",
    " - Try improving fitting time by introducing some **randomness** into your data with the following two model parameters:\n",
    "   - `subsample`: this dictates what proportion of your data will be used for each tree.  A value of `0.7` means 70% of your data will be used for a particular tree, chosen at random\n",
    "   - `max_features`: this is the portion of columns that are used at each individual split.  If you enter an integer the model will randomly select that number of columns, if you enter a decimal it will randomly select that portion of columns.\n",
    "   - It can be very useful to find the most sparse model that will still give you comparable results.  Ie, if you find a gbm with 500 trees and a max_depth of 4 gives you the best results, it can be very beneficial if you can get those same results with a `subsample` value of 0.6 and a `max_features` score of 0.7, because your model will fit ~50% faster.\n",
    "   \n",
    "This step is open ended, so we will likely have to end class in the middle of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model for 250 estimators\n",
      "Model score:  0.9254317507351258\n",
      "Fitting model for 500 estimators\n",
      "Model score:  0.9487882459597867\n"
     ]
    }
   ],
   "source": [
    "# let's look at number of trees first\n",
    "num_trees = [250, 500]\n",
    "\n",
    "for tree in num_trees:\n",
    "    print(f\"Fitting model for {tree} estimators\")\n",
    "    gbm = GradientBoostingRegressor(n_estimators=tree)\n",
    "    gbm.fit(X, y)\n",
    "    print(f\"Model score:  {gbm.score(X, y)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model for max_depth of 4\n",
      "Model score:  0.9223765285137515\n",
      "Fitting model for max_depth of 5\n",
      "Model score:  0.9497649960450486\n",
      "Fitting model for max_depth of 6\n",
      "Model score:  0.9734752030309293\n"
     ]
    }
   ],
   "source": [
    "# and let's look at tree depth\n",
    "tree_depth = [4, 5, 6]\n",
    "# since there was not a huge difference in scores -- let's stick with 100 boosting rounds for now to keep fitting times down\n",
    "for depth in tree_depth:\n",
    "    print(f\"Fitting model for max_depth of {depth}\")\n",
    "    gbm = GradientBoostingRegressor(max_depth=depth)\n",
    "    gbm.fit(X, y)\n",
    "    print(f\"Model score:  {gbm.score(X, y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In each of these cases, we saw modest increases for increasing the value of both parameters.  \n",
    "\n",
    "It would be interesting to see when out of sample scores begin to decrease -- sometimes you can keep increasing these values and keep seeing these piecemeal improvements until your scores get quite a bit higher.  In fact, lots of high performing models are taking existing architectures and just applying a **lot** of horsepower.\n",
    "\n",
    "For now, let's look at the juxtaposition of 500 estimators + `max_depth` of 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9984162960866797"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbm = GradientBoostingRegressor(max_depth=6, n_estimators=500)\n",
    "gbm.fit(X, y)\n",
    "gbm.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This score isn't dramatically different, but we were able to improve our model performance about 15% without a lot of additional effort.\n",
    "\n",
    "Now, let's see if we can recreate similar results by improving fitting times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model with colsample value of 0.3\n",
      "Model score: 0.9941080390699263\n",
      "Fitting model with colsample value of 0.4\n",
      "Model score: 0.9962068158770071\n",
      "Fitting model with colsample value of 0.5\n",
      "Model score: 0.996945495839945\n"
     ]
    }
   ],
   "source": [
    "# first, let's check some different values of colsample -- we'll start with 0.3 -- and go up from there\n",
    "subsample_vals = [0.3, 0.4, 0.5]\n",
    "\n",
    "for num_vals in subsample_vals:\n",
    "    print(f\"Fitting model with colsample value of {num_vals}\")\n",
    "    gbm = GradientBoostingRegressor(subsample=num_vals, n_estimators=500, max_depth=6)\n",
    "    gbm.fit(X, y)\n",
    "    print(f\"Model score: {gbm.score(X, y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scores don't quite get the same values as with all the data -- but they're quite close, and being able to do this with 1/3 of our data is pretty impressive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model with value of 0.3 for max_depth\n",
      "Model score: 0.9857741449962144\n",
      "Fitting model with value of 0.4 for max_depth\n",
      "Model score: 0.9913552305732263\n",
      "Fitting model with value of 0.5 for max_depth\n",
      "Model score: 0.9932716731959361\n",
      "Fitting model with value of 0.6 for max_depth\n",
      "Model score: 0.9932018100696856\n"
     ]
    }
   ],
   "source": [
    "# and let's try some different values for max_features\n",
    "num_cols = [0.3, 0.4, 0.5, 0.6]\n",
    "\n",
    "for num_col in num_cols:\n",
    "    print(f\"Fitting model with value of {num_col} for max_depth\")\n",
    "    gbm = GradientBoostingRegressor(subsample=0.3, n_estimators=500, max_depth=6, max_features=num_col)\n",
    "    gbm.fit(X, y)\n",
    "    print(f\"Model score: {gbm.score(X, y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a similar vein -- we find that reducing the number of columns at a particular split doesn't affect our final results too much.  This is useful because it means we go with a version of our model that uses < 1/6 of the memory of our original, but get very similar results.\n",
    "\n",
    "This can be very useful for using rapid prototypes, where long fitting times can be a drag.  \n",
    "\n",
    "Going with some version of these parameters can be helpful for trying out different versions of our data to see if we can improve our score."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
